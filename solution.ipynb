{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Problem Sheet\n",
    "In this problem sheet I will be using keras with tensorflow to predict the species of Iris from a flowers sepal length and width and a petals length and width.\n",
    "\n",
    "The aim of this problem sheet is to get a better understanding of how tensorflow works.\n",
    "\n",
    "Code for this solution has been adapted from: https://github.com/emerging-technologies/keras-iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is TensorFlow?\n",
    "\n",
    "TensorFlow™ is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. \n",
    "\n",
    "It is an extremely powerful library capable of working with Both CPUs and GPUs. Deep Neural networks are designed, trained and run using this library. \n",
    "\n",
    "In this problem sheet we are going to use Keras API to simplify a lot of stuff for us.\n",
    "\n",
    "### What is Keras?\n",
    "\n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of __TensorFlow__, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go __from idea to result with the least possible delay__ is key to doing good research. If you wish to read more about Keras I'd suggest you read the following article; [Guide to the Functional API](https://keras.io/getting-started/functional-api-guide/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import keras as kr\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris Dataset - what is it?\n",
    "\n",
    "The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. Two of the three species were collected in the Gaspé Peninsula \"all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus\".\n",
    "\n",
    "The Iris data set contains 50 samples each of 3 different species of flowers:\n",
    " - Iris Setosa\n",
    " - Iris Virginica\n",
    " - Iris Versicolor\n",
    " \n",
    "The data has 4 measurements from each sample:\n",
    " - Sepal length\n",
    " - Sepal Width\n",
    " - Petal length\n",
    " - Petal width\n",
    "\n",
    "So let's start with loading and formatting the iris data into a format that tensorflow likes.\n",
    "\n",
    "1. Load the Iris dataset. (Data obtained from: https://github.com/mwaskom/seaborn-data/blob/master/iris.csv)\n",
    "\n",
    "2. The inputs are four floats: sepal length, sepal width, petal length, petal width.\n",
    "\n",
    "3. Outputs are initially individual strings: setosa, versicolor or virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = list(csv.reader(open('iris.csv')))[1:]\n",
    "inputs  = np.array(iris)[:,:4].astype(np.float)\n",
    "outputs = np.array(iris)[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are caterogical variables?\n",
    "\n",
    "In many practical Data Science activities, the data set will contain categorical variables. These variables are typically stored as text values which represent various traits. Some examples include color (“Red”, “Yellow”, “Blue”), size (“Small”, “Medium”, “Large”) or geographic designations (State or Country). Regardless of what the value is used for, the challenge is determining how to use this data in the analysis. \n",
    "\n",
    "We are going to use Keras to encode integers as binary caterogical variables using __to_categorical__ method.\n",
    "\n",
    "### to_caterogical method\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">keras.utils.to_categorical(y, num_classes=None)</div>\n",
    "\n",
    "Converts a class vector (integers) to binary class matrix.\n",
    "\n",
    "E.g. for use with categorical_crossentropy.\n",
    "\n",
    "#### Arguments\n",
    "\n",
    "* y: class vector to be converted into a matrix (integers from 0 to num_classes).\n",
    "* num_classes: total number of classes.\n",
    "\n",
    "#### Returns\n",
    "\n",
    "* A binary matrix representation of the input.\n",
    "\n",
    "\n",
    "So, first of all we are going to convert the output strings to integers, and then we will encode the category integers as binary categorical variables using the above method. Then, we are going to split the input and output data sets into training and test subsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs_vals, outputs_ints = np.unique(outputs, return_inverse=True)\n",
    "outputs_cats = kr.utils.to_categorical(outputs_ints)\n",
    "inds = np.random.permutation(len(inputs))\n",
    "train_inds, test_inds = np.array_split(inds, 2)\n",
    "inputs_train, outputs_train = inputs[train_inds], outputs_cats[train_inds]\n",
    "inputs_test,  outputs_test  = inputs[test_inds],  outputs_cats[test_inds]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the neural network\n",
    "\n",
    "Neural Networks have been in the spotlight for quite some time now. For a more detailed explanation on neural network and deep learning read [here](https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/). Its “deeper” versions are making tremendous breakthroughs in many fields such as image recognition, speech and natural language processing etc.\n",
    "\n",
    "The main question that arises is when to and when not to apply neural networks? You have to keep a few things in mind:\n",
    "\n",
    "__Firstly, neural networks require clear and informative data (and mostly big data) to train.__ Try to imagine Neural Networks as a child. It first observes how its parent walks. Then it tries to walk on its own, and with its every step, the child learns how to perform a particular task. It may fall a few times, but after few unsuccessful attempts, it learns how to walk. If you don’t let it walk, it might not ever learn how to walk. The more exposure you can provide to the child, the better it is.\n",
    "\n",
    "__It is prudent to use Neural Networks for complex problems such as image processing.__ Neural nets belong to a class of algorithms called representation learning algorithms. These algorithms break down complex problems into simpler form so that they become understandable (or “representable”).\n",
    "\n",
    "__When you have appropriate type of neural network to solve the problem.__ Each problem has its own twists. So the data decides the way you solve the problem. For example, if the problem is of sequence generation, recurrent neural networks are more suitable. Whereas, if it is image related problem, you would probably be better of taking convolutional neural networks for a change.\n",
    "\n",
    "__Last but not the least, hardware requirements are essential for running a deep neural network model.__ Neural nets were “discovered” long ago, but they are shining in the recent years for the main reason that computational resources are better and more powerful. If you want to solve a real life problem with these networks, get ready to buy some high-end hardware!\n",
    "\n",
    "![Graph](https://www.analyticsvidhya.com/wp-content/uploads/2016/08/Artificial-Intelligence-Neural-Network-Nodes-670x440.jpg)\n",
    "\n",
    "Now we will create our own neural network for Iris dataset. We will create with creating a model, add initial layer with 4 nodes and a hidden layer with 16 nodes. We will apply the sigmoid activation function to that layer. But wait, what's sigmoid activation? A sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve. Often, sigmoid function refers to the special case of the logistic function shown in the first figure and defined by the formula:\n",
    "\n",
    "![formula image](https://wikimedia.org/api/rest_v1/media/math/render/svg/9537e778e229470d85a68ee0b099c08298a1a3f6)\n",
    "\n",
    "We will add another layer, connected to the one with 16 nodes, containing three output nodes. Lastly we will use softmax activation function there. \n",
    "\n",
    "What is softmax function?\n",
    "\n",
    "Softmax function calculates the probabilities distribution of the event over ‘n’ different events. In general way of saying, this function will calculate the probabilities of each target class over all possible target classes. Later the calculated probabilities will be helpful for determining the target class for the given inputs.\n",
    "\n",
    "What's the difference between them two?\n",
    "\n",
    "* Softmax: Used for the multi-classification task.\n",
    "* Sigmoid: Used for the binary classification task.\n",
    "\n",
    "![difference](https://i0.wp.com/dataaspirant.com/wp-content/uploads/2017/03/SigmoidVsSoftmax-compressor.jpg?w=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = kr.models.Sequential()\n",
    "model.add(kr.layers.Dense(16, input_shape=(4,)))\n",
    "model.add(kr.layers.Activation(\"sigmoid\"))\n",
    "model.add(kr.layers.Dense(3))\n",
    "model.add(kr.layers.Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will next configure the model for training. \n",
    "\n",
    "* Uses the adam optimizer and categorical cross entropy as the loss function.\n",
    "\n",
    "* Add in some extra metrics - accuracy being the only one.\n",
    "\n",
    "#### What's adam optimizer?\n",
    "\n",
    "Adam is an optimization algorithm that can used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data.\n",
    "\n",
    "Adam was presented by Diederik Kingma from OpenAI and Jimmy Ba from the University of Toronto in their 2015 ICLR paper (poster) titled “Adam: A Method for Stochastic Optimization“. I will quote liberally from their paper in this post, unless stated otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fit the model using our training data.\n",
    "* Evaluate the model using the test data set.\n",
    "* Output the accuracy of the model.\n",
    "\n",
    "### fit \n",
    "Trains the model for a fixed number of epochs (iterations on a dataset).\n",
    "\n",
    "### evaluate\n",
    "Returns the loss value & metrics values for the model in test mode.\n",
    "\n",
    "Computation is done in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "75/75 [==============================] - 0s - loss: 0.0215 - acc: 1.0000       \n",
      "Epoch 2/100\n",
      "75/75 [==============================] - 0s - loss: 0.0203 - acc: 0.9867     \n",
      "Epoch 3/100\n",
      "75/75 [==============================] - 0s - loss: 0.0207 - acc: 0.9867     \n",
      "Epoch 4/100\n",
      "75/75 [==============================] - 0s - loss: 0.0222 - acc: 1.0000     \n",
      "Epoch 5/100\n",
      "75/75 [==============================] - 0s - loss: 0.0222 - acc: 0.9867     \n",
      "Epoch 6/100\n",
      "75/75 [==============================] - 0s - loss: 0.0204 - acc: 0.9867     \n",
      "Epoch 7/100\n",
      "75/75 [==============================] - 0s - loss: 0.0196 - acc: 1.0000        \n",
      "Epoch 8/100\n",
      "75/75 [==============================] - 0s - loss: 0.0202 - acc: 0.9867       \n",
      "Epoch 9/100\n",
      "75/75 [==============================] - 0s - loss: 0.0198 - acc: 0.9867       \n",
      "Epoch 10/100\n",
      "75/75 [==============================] - 0s - loss: 0.0214 - acc: 0.9867       \n",
      "Epoch 11/100\n",
      "75/75 [==============================] - 0s - loss: 0.0174 - acc: 1.0000     \n",
      "Epoch 12/100\n",
      "75/75 [==============================] - 0s - loss: 0.0190 - acc: 0.9867       \n",
      "Epoch 13/100\n",
      "75/75 [==============================] - 0s - loss: 0.0169 - acc: 1.0000       \n",
      "Epoch 14/100\n",
      "75/75 [==============================] - 0s - loss: 0.0194 - acc: 0.9867     \n",
      "Epoch 15/100\n",
      "75/75 [==============================] - 0s - loss: 0.0199 - acc: 0.9867     \n",
      "Epoch 16/100\n",
      "75/75 [==============================] - 0s - loss: 0.0201 - acc: 0.9867        \n",
      "Epoch 17/100\n",
      "75/75 [==============================] - 0s - loss: 0.0208 - acc: 1.0000        \n",
      "Epoch 18/100\n",
      "75/75 [==============================] - 0s - loss: 0.0212 - acc: 0.9867       \n",
      "Epoch 19/100\n",
      "75/75 [==============================] - 0s - loss: 0.0203 - acc: 0.9867     \n",
      "Epoch 20/100\n",
      "75/75 [==============================] - 0s - loss: 0.0167 - acc: 1.0000       \n",
      "Epoch 21/100\n",
      "75/75 [==============================] - 0s - loss: 0.0197 - acc: 1.0000       \n",
      "Epoch 22/100\n",
      "75/75 [==============================] - 0s - loss: 0.0200 - acc: 1.0000        \n",
      "Epoch 23/100\n",
      "75/75 [==============================] - 0s - loss: 0.0205 - acc: 1.0000       \n",
      "Epoch 24/100\n",
      "75/75 [==============================] - 0s - loss: 0.0215 - acc: 0.9867       \n",
      "Epoch 25/100\n",
      "75/75 [==============================] - 0s - loss: 0.0219 - acc: 1.0000       \n",
      "Epoch 26/100\n",
      "75/75 [==============================] - 0s - loss: 0.0194 - acc: 0.9867     \n",
      "Epoch 27/100\n",
      "75/75 [==============================] - 0s - loss: 0.0185 - acc: 1.0000       \n",
      "Epoch 28/100\n",
      "75/75 [==============================] - 0s - loss: 0.0207 - acc: 1.0000       \n",
      "Epoch 29/100\n",
      "75/75 [==============================] - 0s - loss: 0.0192 - acc: 1.0000     \n",
      "Epoch 30/100\n",
      "75/75 [==============================] - 0s - loss: 0.0208 - acc: 0.9867     \n",
      "Epoch 31/100\n",
      "75/75 [==============================] - 0s - loss: 0.0196 - acc: 0.9867     \n",
      "Epoch 32/100\n",
      "75/75 [==============================] - 0s - loss: 0.0190 - acc: 1.0000       \n",
      "Epoch 33/100\n",
      "75/75 [==============================] - 0s - loss: 0.0186 - acc: 0.9867       \n",
      "Epoch 34/100\n",
      "75/75 [==============================] - 0s - loss: 0.0192 - acc: 0.9867       \n",
      "Epoch 35/100\n",
      "75/75 [==============================] - 0s - loss: 0.0193 - acc: 1.0000       \n",
      "Epoch 36/100\n",
      "75/75 [==============================] - 0s - loss: 0.0177 - acc: 1.0000       \n",
      "Epoch 37/100\n",
      "75/75 [==============================] - 0s - loss: 0.0211 - acc: 0.9867       \n",
      "Epoch 38/100\n",
      "75/75 [==============================] - 0s - loss: 0.0201 - acc: 1.0000     \n",
      "Epoch 39/100\n",
      "75/75 [==============================] - 0s - loss: 0.0191 - acc: 0.9867       \n",
      "Epoch 40/100\n",
      "75/75 [==============================] - 0s - loss: 0.0207 - acc: 0.9867     \n",
      "Epoch 41/100\n",
      "75/75 [==============================] - 0s - loss: 0.0191 - acc: 1.0000       \n",
      "Epoch 42/100\n",
      "75/75 [==============================] - 0s - loss: 0.0182 - acc: 0.9867       \n",
      "Epoch 43/100\n",
      "75/75 [==============================] - 0s - loss: 0.0211 - acc: 0.9867       \n",
      "Epoch 44/100\n",
      "75/75 [==============================] - 0s - loss: 0.0181 - acc: 1.0000     \n",
      "Epoch 45/100\n",
      "75/75 [==============================] - 0s - loss: 0.0171 - acc: 1.0000       \n",
      "Epoch 46/100\n",
      "75/75 [==============================] - 0s - loss: 0.0186 - acc: 1.0000        \n",
      "Epoch 47/100\n",
      "75/75 [==============================] - 0s - loss: 0.0187 - acc: 0.9867        \n",
      "Epoch 48/100\n",
      "75/75 [==============================] - 0s - loss: 0.0160 - acc: 1.0000     \n",
      "Epoch 49/100\n",
      "75/75 [==============================] - 0s - loss: 0.0179 - acc: 1.0000       \n",
      "Epoch 50/100\n",
      "75/75 [==============================] - 0s - loss: 0.0210 - acc: 0.9867     \n",
      "Epoch 51/100\n",
      "75/75 [==============================] - 0s - loss: 0.0190 - acc: 1.0000     \n",
      "Epoch 52/100\n",
      "75/75 [==============================] - 0s - loss: 0.0176 - acc: 0.9867     \n",
      "Epoch 53/100\n",
      "75/75 [==============================] - 0s - loss: 0.0182 - acc: 1.0000        \n",
      "Epoch 54/100\n",
      "75/75 [==============================] - 0s - loss: 0.0198 - acc: 0.9867       \n",
      "Epoch 55/100\n",
      "75/75 [==============================] - 0s - loss: 0.0232 - acc: 0.9867       \n",
      "Epoch 56/100\n",
      "75/75 [==============================] - 0s - loss: 0.0196 - acc: 1.0000     \n",
      "Epoch 57/100\n",
      "75/75 [==============================] - 0s - loss: 0.0189 - acc: 0.9867     \n",
      "Epoch 58/100\n",
      "75/75 [==============================] - 0s - loss: 0.0186 - acc: 1.0000       \n",
      "Epoch 59/100\n",
      "75/75 [==============================] - 0s - loss: 0.0186 - acc: 0.9867     \n",
      "Epoch 60/100\n",
      "75/75 [==============================] - 0s - loss: 0.0184 - acc: 0.9867     \n",
      "Epoch 61/100\n",
      "75/75 [==============================] - 0s - loss: 0.0160 - acc: 1.0000        \n",
      "Epoch 62/100\n",
      "75/75 [==============================] - 0s - loss: 0.0172 - acc: 1.0000       \n",
      "Epoch 63/100\n",
      "75/75 [==============================] - 0s - loss: 0.0178 - acc: 1.0000       \n",
      "Epoch 64/100\n",
      "75/75 [==============================] - 0s - loss: 0.0163 - acc: 1.0000       \n",
      "Epoch 65/100\n",
      "75/75 [==============================] - 0s - loss: 0.0187 - acc: 0.9867     \n",
      "Epoch 66/100\n",
      "75/75 [==============================] - 0s - loss: 0.0191 - acc: 0.9867       \n",
      "Epoch 67/100\n",
      "75/75 [==============================] - 0s - loss: 0.0161 - acc: 1.0000     \n",
      "Epoch 68/100\n",
      "75/75 [==============================] - 0s - loss: 0.0169 - acc: 1.0000        \n",
      "Epoch 69/100\n",
      "75/75 [==============================] - 0s - loss: 0.0177 - acc: 1.0000       \n",
      "Epoch 70/100\n",
      "75/75 [==============================] - 0s - loss: 0.0183 - acc: 0.9867       \n",
      "Epoch 71/100\n",
      "75/75 [==============================] - 0s - loss: 0.0186 - acc: 0.9867       \n",
      "Epoch 72/100\n",
      "75/75 [==============================] - 0s - loss: 0.0186 - acc: 1.0000     \n",
      "Epoch 73/100\n",
      "75/75 [==============================] - 0s - loss: 0.0198 - acc: 0.9867        \n",
      "Epoch 74/100\n",
      "75/75 [==============================] - 0s - loss: 0.0152 - acc: 1.0000       \n",
      "Epoch 75/100\n",
      "75/75 [==============================] - 0s - loss: 0.0169 - acc: 1.0000     \n",
      "Epoch 76/100\n",
      "75/75 [==============================] - 0s - loss: 0.0168 - acc: 1.0000        \n",
      "Epoch 77/100\n",
      "75/75 [==============================] - 0s - loss: 0.0174 - acc: 1.0000     \n",
      "Epoch 78/100\n",
      "75/75 [==============================] - 0s - loss: 0.0187 - acc: 1.0000       \n",
      "Epoch 79/100\n",
      "75/75 [==============================] - 0s - loss: 0.0150 - acc: 1.0000       \n",
      "Epoch 80/100\n",
      "75/75 [==============================] - 0s - loss: 0.0178 - acc: 1.0000       \n",
      "Epoch 81/100\n",
      "75/75 [==============================] - 0s - loss: 0.0186 - acc: 1.0000       \n",
      "Epoch 82/100\n",
      "75/75 [==============================] - 0s - loss: 0.0159 - acc: 1.0000     \n",
      "Epoch 83/100\n",
      "75/75 [==============================] - 0s - loss: 0.0192 - acc: 0.9867       \n",
      "Epoch 84/100\n",
      "75/75 [==============================] - 0s - loss: 0.0191 - acc: 1.0000     \n",
      "Epoch 85/100\n",
      "75/75 [==============================] - 0s - loss: 0.0164 - acc: 1.0000       \n",
      "Epoch 86/100\n",
      "75/75 [==============================] - 0s - loss: 0.0176 - acc: 0.9867       \n",
      "Epoch 87/100\n",
      "75/75 [==============================] - 0s - loss: 0.0174 - acc: 1.0000       \n",
      "Epoch 88/100\n",
      "75/75 [==============================] - 0s - loss: 0.0185 - acc: 1.0000       \n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s - loss: 0.0207 - acc: 0.9867     \n",
      "Epoch 90/100\n",
      "75/75 [==============================] - 0s - loss: 0.0193 - acc: 1.0000     \n",
      "Epoch 91/100\n",
      "75/75 [==============================] - 0s - loss: 0.0179 - acc: 1.0000        \n",
      "Epoch 92/100\n",
      "75/75 [==============================] - 0s - loss: 0.0156 - acc: 1.0000     \n",
      "Epoch 93/100\n",
      "75/75 [==============================] - 0s - loss: 0.0142 - acc: 1.0000     \n",
      "Epoch 94/100\n",
      "75/75 [==============================] - 0s - loss: 0.0190 - acc: 0.9867     \n",
      "Epoch 95/100\n",
      "75/75 [==============================] - 0s - loss: 0.0163 - acc: 1.0000        \n",
      "Epoch 96/100\n",
      "75/75 [==============================] - 0s - loss: 0.0170 - acc: 1.0000        \n",
      "Epoch 97/100\n",
      "75/75 [==============================] - 0s - loss: 0.0168 - acc: 1.0000     \n",
      "Epoch 98/100\n",
      "75/75 [==============================] - 0s - loss: 0.0166 - acc: 1.0000        \n",
      "Epoch 99/100\n",
      "75/75 [==============================] - 0s - loss: 0.0176 - acc: 1.0000        \n",
      "Epoch 100/100\n",
      "75/75 [==============================] - 0s - loss: 0.0156 - acc: 0.9867        \n",
      "32/75 [===========>..................] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "model.fit(inputs_train, outputs_train, epochs=100, batch_size=1, verbose=1)\n",
    "loss, accuracy = model.evaluate(inputs_test, outputs_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loss: 0.1053\tAccuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nLoss: %6.4f\\tAccuracy: %6.4f\" % (loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the class of a single flower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: [0 1 0]\tEstimated: [0 1 0]\n",
      "That means it's a versicolor\n"
     ]
    }
   ],
   "source": [
    "prediction = np.around(model.predict(np.expand_dims(inputs_test[0], axis=0))).astype(np.int)[0]\n",
    "print(\"Actual: %s\\tEstimated: %s\" % (outputs_test[0].astype(np.int), prediction))\n",
    "print(\"That means it's a %s\" % outputs_vals[prediction.astype(np.bool)][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to a file for later use, and try loading it from saved file.\n",
    "\n",
    "### Saving models\n",
    "How to save a Keras model?\n",
    "\n",
    "Saving/loading whole models (architecture + weights + optimizer state)\n",
    "\n",
    "It is not recommended to use pickle or cPickle to save a Keras model.\n",
    "\n",
    "We can use __model.save(filepath)__ to save a Keras model into a single HDF5 file which will contain:\n",
    "\n",
    "* the architecture of the model, allowing to re-create the model\n",
    "* the weights of the model\n",
    "* the training configuration (loss, optimizer)\n",
    "* the state of the optimizer, allowing to resume training exactly where you left off.\n",
    "\n",
    "You can then use __keras.models.load_model(filepath)__ to reinstantiate your model. load_model will also take care of compiling the model using the saved training configuration (unless the model was never compiled in the first place)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"iris_nn.h5\")\n",
    "model = load_model(\"iris_nn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References used for this problem sheet\n",
    "\n",
    "1. https://www.analyticsvidhya.com/blog/2016/10/an-introduction-to-implementing-neural-networks-using-tensorflow/\n",
    "2. https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/\n",
    "3. https://en.wikipedia.org/wiki/Sigmoid_function\n",
    "4. http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/\n",
    "5. https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n",
    "6. https://keras.io/models/model/\n",
    "7. https://keras.io/getting-started/faq/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
